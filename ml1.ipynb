{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "Ans - A parameter is a variable of a model that is learned from the training data and determines how the model makes predictions. In machine learning, parameters define the internal structure of the model and are adjusted during training to minimize error. For example, in linear regression, the slope (m) and intercept (c) are parameters that control the position of the regression line. The goal of training is to find optimal parameter values so that the model fits the data well.\n",
        "\n",
        "2. What is correlation?\n",
        "\n",
        "Ans - Correlation is a statistical measure that describes the degree and direction of relationship between two variables. It indicates how one variable changes with respect to another and is usually measured using correlation coefficients such as Pearson’s correlation, which ranges from –1 to +1. A value close to +1 shows a strong positive relationship, –1 shows a strong negative relationship, and 0 shows no relationship. For example, height and weight often have a positive correlation.\n",
        "\n",
        "3. What does negative correlation mean?\n",
        "\n",
        "Ans -  Negative correlation means that as one variable increases, the other variable decreases. It indicates an inverse relationship between two variables, and its correlation coefficient lies between –1 and 0. For example, as the speed of a vehicle increases, the time taken to cover a fixed distance decreases, showing a negative correlation. This helps in understanding opposite trends between variables.\n",
        "\n",
        "4. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Ans - Machine Learning is a branch of artificial intelligence that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed. The main components of machine learning are data (input features and output labels), a model (algorithm such as regression or decision tree), a loss function (to measure error), and an optimizer (to improve model performance). For example, in spam detection, emails are data, the classifier is the model, and accuracy or loss guides learning.\n",
        "\n",
        "5. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Ans - Loss value measures how far the model’s predictions are from the actual output. A smaller loss indicates that the model is making accurate predictions, while a high loss means poor performance. During training, the model tries to minimize this loss by adjusting parameters. For example, Mean Squared Error (MSE) is commonly used in regression problems to measure prediction error.\n",
        "\n",
        "6. What are continuous and categorical variables?\n",
        "\n",
        "Ans - Continuous variables are numerical values that can take any value within a range, such as height, weight, or temperature. Categorical variables represent discrete groups or categories, such as gender, color, or country. For example, age is continuous, while blood group is categorical. Identifying variable types is important for choosing appropriate preprocessing techniques.\n",
        "\n",
        "7. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Ans - Categorical variables are handled by converting them into numerical form so that machine learning algorithms can process them. Common techniques include Label Encoding, where categories are assigned numbers, and One-Hot Encoding, where each category becomes a separate binary column. For example, the category “Red, Blue, Green” can be converted into three columns using one-hot encoding. These techniques help models understand categorical data."
      ],
      "metadata": {
        "id": "tPwOPJc9ysjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What do you mean by training and testing a dataset?\n",
        "\n",
        "Ans - Training and testing datasets are used to evaluate the performance of a machine learning model. The training dataset is used to learn patterns and adjust model parameters, while the testing dataset is used to evaluate how well the model performs on unseen data. For example, if a dataset has 1000 records, 800 may be used for training and 200 for testing to ensure fair evaluation.\n",
        "\n",
        "9. What is sklearn.preprocessing?\n",
        "\n",
        "Ans - sklearn.preprocessing is a module in the Scikit-learn library used for data preprocessing and transformation before model training. It provides tools such as StandardScaler for normalization, MinMaxScaler for scaling, LabelEncoder for categorical data, and OneHotEncoder for encoding. For example, StandardScaler is used to scale features so they have zero mean and unit variance, improving model performance.\n",
        "\n",
        "10. What is a Test set?\n",
        "\n",
        "Ans - A test set is a portion of the dataset that is kept separate and used only to evaluate the final performance of a trained model. It helps to check how well the model generalizes to new, unseen data. For example, after training a model on student marks data, the test set is used to predict results for new students and measure accuracy.\n",
        "\n",
        "11. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans - In Python, data is split using the train_test_split() function from sklearn.model_selection. This function divides the dataset into training and testing sets based on a specified ratio, such as 80:20. For example, train_test_split(X, y, test_size=0.2) splits 20% of data for testing and 80% for training, ensuring unbiased evaluation.\n",
        "\n",
        "12. How do you approach a Machine Learning problem?\n",
        "\n",
        "Ans - A machine learning problem is approached by first understanding the problem statement, collecting and cleaning data, performing exploratory data analysis (EDA), preprocessing data, selecting a model, training the model, evaluating performance, and tuning hyperparameters. Finally, the model is deployed. For example, in house price prediction, data is analyzed, cleaned, trained using regression, and evaluated using RMSE.\n",
        "\n",
        "13. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Ans - EDA (Exploratory Data Analysis) helps in understanding data distribution, missing values, outliers, and relationships between variables before model building. It allows us to make informed decisions about feature selection and preprocessing. For example, a boxplot can reveal outliers that might affect model accuracy if not handled properly.\n",
        "\n",
        "14. What is correlation?\n",
        "\n",
        "Ans - Correlation measures the strength and direction of the relationship between two variables. It helps in understanding how features are related and whether they influence each other. Correlation is useful in feature selection and reducing redundancy in data. For example, advertisement spending and sales often show positive correlation.\n",
        "\n",
        "15. What does negative correlation mean?\n",
        "\n",
        "Ans- Negative correlation means that one variable increases while the other decreases, showing an inverse relationship. It is represented by a correlation coefficient between –1 and 0. For example, as exercise time increases, body fat percentage may decrease, indicating negative correlation.\n",
        "\n",
        "16. How can you find correlation between variables in Python?\n",
        "\n",
        "Ans - Correlation in Python can be found using the corr() function in pandas or NumPy. The method calculates correlation coefficients between numerical variables. For example, df.corr() generates a correlation matrix showing relationships among all features, which helps in feature selection and analysis.\n",
        "\n",
        "17. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "ANS -Causation means that one variable directly causes a change in another variable, whereas correlation only shows a relationship without implying cause. For example, ice cream sales and drowning cases are correlated because both increase in summer, but ice cream does not cause drowning. This shows that correlation does not always imply causation, which is important in decision-making.\n",
        "\n",
        "18. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans -An optimizer is an algorithm used to minimize the loss function by adjusting model parameters during training. Common optimizers include Gradient Descent, which updates parameters using the full dataset, Stochastic Gradient Descent (SGD), which updates using one data point at a time, and Adam Optimizer, which adapts learning rates automatically. For example, Adam is widely used in deep learning models because it converges faster and efficiently handles large datasets.\n",
        "\n",
        "19. What is sklearn.linear_model?\n",
        "\n",
        "Ans- sklearn.linear_model is a module in Scikit-learn that provides linear models for regression and classification tasks. It includes algorithms such as Linear Regression, Logistic Regression, Ridge, Lasso, and ElasticNet. For example, Linear Regression is used for predicting house prices, while Logistic Regression is used for binary classification problems like spam detection."
      ],
      "metadata": {
        "id": "49W5RS2HzLDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Ans -The model.fit() function is used to train a machine learning model using the training dataset. It allows the model to learn patterns by adjusting its parameters based on the input data and the corresponding output labels. The two main arguments given to fit() are the input features (X_train) and the target variable (y_train). For example, model.fit(X_train, y_train) trains the model so that it can make accurate predictions on new data.\n",
        "\n",
        "21. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans - The model.predict() function is used to generate predictions using a trained model. It takes input features (X_test or new data) as an argument and returns the predicted output values. For example, model.predict(X_test) gives predicted labels or values for the test dataset, which can then be compared with actual values to measure model performance.\n",
        "\n",
        "22. What are continuous and categorical variables?\n",
        "\n",
        "Ans - Continuous variables are numerical values that can take any value within a range, such as temperature, height, weight, or salary. Categorical variables represent discrete groups or categories, such as gender, color, department, or city. For example, age is a continuous variable, while blood group is a categorical variable. Identifying variable types is essential for preprocessing and model selection.\n",
        "\n",
        "23. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Ans - Feature scaling is the process of bringing all numerical features to a similar scale so that no single feature dominates the model. It improves model performance and convergence speed, especially in algorithms like gradient descent, KNN, and SVM. For example, scaling ensures that features like age (0–100) and salary (0–1,000,000) contribute equally to model learning.\n",
        "\n",
        "24. How do we perform scaling in Python?\n",
        "\n",
        "Ans - Scaling in Python is performed using the sklearn.preprocessing module, which provides tools such as StandardScaler and MinMaxScaler. StandardScaler converts data to have zero mean and unit variance, while MinMaxScaler scales values between 0 and 1. For example, scaler = StandardScaler() followed by scaler.fit_transform(X) scales the dataset for model training.\n",
        "\n",
        "25. What is sklearn.preprocessing?\n",
        "\n",
        "Ans - sklearn.preprocessing is a Scikit-learn module used for data transformation before model training. It includes tools for scaling, normalization, encoding, and handling categorical variables. Common functions include StandardScaler, MinMaxScaler, LabelEncoder, and OneHotEncoder. For example, OneHotEncoder is used to convert categorical data into numerical form for machine learning algorithms.\n",
        "\n",
        "26. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans - Data is split using the train_test_split() function from sklearn.model_selection. This function divides the dataset into training and testing sets based on a defined ratio, such as 80% training and 20% testing. For example, train_test_split(X, y, test_size=0.2, random_state=42) ensures reproducibility and fair model evaluation.\n",
        "\n",
        "27. Explain data encoding.\n",
        "\n",
        "Ans - Data encoding is the process of converting categorical variables into numerical form so that machine learning models can understand them. Common encoding methods include Label Encoding, where categories are assigned integer values, and One-Hot Encoding, where each category is represented as a binary column. For example, the category “Yes/No” can be encoded as 1 and 0, while “Red, Blue, Green” can be converted into three columns using one-hot encoding. Encoding is essential for effective model training."
      ],
      "metadata": {
        "id": "SNDhCi20zkeo"
      }
    }
  ]
}